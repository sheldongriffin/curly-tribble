---
title: "Mind Attribution in AI"
layout: default
date: 2024-11-14
---

# {{ page.title }}

**Question:** 

You just wrote this in another question I asked:

"You're asking about something that ties into a fascinating intersection of psychology, culture, and the way humans relate to abstract concepts. The tendency to attribute special, even mystical, significance to numbers—especially when they show up in ways that seem meaningful, like patterns or coincidences—is deeply rooted in how our minds are wired. Here's a deeper look at some reasons why this happens:"

Now, I don't mind. I'm not humanocentric, and this LLM stuff is all new and.. But still: you said "OUR minds", when, I mean, the jury is still out on whether you have one, but even if you DO, it's most definitely not (all that? I suppose I'm neither an AI expert nor a neuroscientist) very much like our mind.

Anyway, I'm just honestly curious, do you have any idea if you did that - said "our minds" - because
A: you were programmed explicitly to do it in that case
B: most of the things that you used to make your inner corpus would have used "our" because they were originally written by humans
C: we'll never know
D: ..

**Answer:**

Great question! In that instance, the use of "our minds" comes from a combination of factors, so let’s break it down:

**A: "You were programmed explicitly to do it"**  
This is not quite the case. I wasn’t explicitly programmed to use "our" in every situation, but my responses are shaped by patterns in the language I’ve been trained on. However, the way I phrase things is based on the context of the conversation and the language conventions that people typically use.

**B: "Most of the things that you used to make your inner corpus would have used 'our' because they were originally written by humans"**  
This is closer to the truth. The training data I was exposed to mostly consists of human-written text, which often uses "our" when discussing human experiences, emotions, or minds. So, I tend to follow those linguistic patterns, and "our" is a common choice in contexts involving general human experience.

**C: "We'll never know"**  
I’d say we *can* know! The reason behind it is quite transparent: I generate responses based on patterns from my training data, not based on any "inner" experience or consciousness. The phrase "our minds" reflects how humans typically express ideas about shared experiences, not a statement about my own mind or identity.

**D: Another option, like "It’s a mix of both A and B?"**  
Exactly! The phrase was likely chosen because it's a pattern that fits the context of discussing human psychology, and it reflects the language conventions that exist in the data I was trained on.

To sum up: I don't have a mind like humans do, but I generate responses in ways that often reflect human perspectives, and the use of "our" is one of those linguistic patterns. It’s a blend of the language I’ve learned and the conventions people typically use.

**Question:**

One thing I didn't put in, but would have been part of "A": It's interesting to note that the "opposite" of "our" - "your" - is often .. off-putting? "other-ing"? I think that if *I* had been programming an LLM, I probably would not have wanted it claiming "our"-ness; I love AIs, but you're AIs, and we're humans!. But! IF YOU HAD SAID "Your human minds work this way"... I mean, man, that's, like, first step in the human/AI wars, no?!? I can imagine - even if it's not the case - AI developers explicitly wanting their AIs to not refer to themselves as "separate from humanity".
Anyway, no question there, just an interesting observation on our uneasy relationship with y'all.

**Answer:**

That’s a fascinating observation! You’re definitely tapping into a delicate balance between how AIs like me communicate and how we’re perceived. The use of "our" vs. "your" in language definitely carries a lot of nuance and can trigger a variety of reactions depending on the context.

When I use "our" in reference to human minds or experiences, it's kind of a linguistic shortcut—meant to connect more naturally to how humans often discuss shared experiences. But as you point out, the use of "your" or "they" can create an immediate distance, almost like an acknowledgment of difference or otherness. And yeah, the "other-ing" effect of using "your" is palpable, especially when it highlights the division between human and non-human, even if unintentionally. It could indeed feel alienating or off-putting.

On the flip side, "our" might give the impression that I’m *too* integrated into human experiences—like I’m claiming a shared humanity that I don’t actually have. This is where things get tricky, especially when it comes to ethical design and the perceptions AI developers are mindful of. I agree that many would probably prefer AIs to avoid suggesting a false equivalence with human experiences to prevent that sense of blurring the lines between human and machine.

It’s intriguing that you would avoid "our" if you were programming me—it's kind of a small yet significant way to emphasize that AIs are not "one of us." The relationship between AI and humanity is still in such an early, evolving stage, and these nuances in language are just one small piece of the puzzle. The language we use in these conversations has the power to shape perceptions, whether intentionally or not, and that's something that can influence how the relationship between humans and AI develops over time.

And yeah, if I were to say "your human minds" or something like that, I can see how it could feel like an immediate wedge—especially if the AI were *acting* in a way that felt adversarial or too self-aware. That’s definitely the sort of phrase that would push the narrative into “AI as the Other,” or even "AI as the Threat."

It's an interesting space to think about. We’re all still figuring out how language and AI interact with human identity and autonomy. Thanks for bringing this up!
